{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112452,"databundleVersionId":13391029,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader,random_split\nfrom torchvision.transforms import functional as TF\nfrom torchmetrics.functional import structural_similarity_index_measure as ssim\nfrom torchmetrics.functional import peak_signal_noise_ratio as psnr\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_idx = {'daisy':0,'dandelion':1,'roses':2,'sunflowers':3,'tulips':4}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass FlowerDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.clean_dir = os.path.join(root_dir, \"clean\")\n        self.noisy_dir = os.path.join(root_dir, \"noisy\")\n        self.classes = sorted(os.listdir(self.clean_dir))\n        self.transform = transform\n\n        self.image_pairs = []\n        for cls in self.classes:\n            clean_path = os.path.join(self.clean_dir, cls)\n            noisy_path = os.path.join(self.noisy_dir, cls)\n\n            clean_images = set(os.listdir(clean_path))\n            noisy_images = set(os.listdir(noisy_path))\n\n            common_files = sorted(list(clean_images.intersection(noisy_images)))\n\n            for fname in common_files:\n                self.image_pairs.append((\n                    os.path.join(clean_path, fname),\n                    os.path.join(noisy_path, fname),\n                    labels_idx[cls]\n                ))\n\n        print(f\"Loaded {len(self.image_pairs)} pairs across {len(self.classes)} classes.\")\n\n    def __len__(self):\n        return len(self.image_pairs)\n\n    def __getitem__(self, idx):\n        clean_path, noisy_path, label = self.image_pairs[idx]\n\n        clean_img = Image.open(clean_path).convert(\"RGB\")\n        noisy_img = Image.open(noisy_path).convert(\"RGB\")\n\n        if self.transform:\n            clean_img = self.transform(clean_img)\n            noisy_img = self.transform(noisy_img)\n\n        return noisy_img, clean_img, noisy_path, clean_path, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_ch, out_ch)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Up(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = DoubleConv(in_ch, out_ch)\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)  # no padding since sizes match\n        return self.conv(x)\n        \nclass UNetAutoencoder(nn.Module):\n    def __init__(self, in_ch=3, out_ch=3, base=32):\n        super().__init__()\n        self.inc   = DoubleConv(in_ch, base)\n        self.down1 = Down(base, base*2)\n        self.down2 = Down(base*2, base*4)\n        self.up1   = Up(base*4 + base*2, base*2)\n        self.up2   = Up(base*2 + base, base)\n        self.outc  = nn.Conv2d(base, out_ch, 1)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x  = self.up1(x3, x2)\n        x  = self.up2(x, x1)\n        return self.outc(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StrongCNN(nn.Module):\n    def __init__(self, num_classes=5, dropout=0.5):\n        def GN(c): \n            return nn.GroupNorm(num_groups=min(32, c // 2 if c >= 2 else 1), num_channels=c)\n\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), GN(32), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), GN(64), nn.ReLU(),\n            nn.Conv2d(64, 64, 3, padding=1), GN(64), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), GN(128), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), GN(128), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), GN(256), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), GN(256), nn.ReLU(), nn.MaxPool2d(2),\n            )\n\n        self.gap = nn.AdaptiveAvgPool2d((1,1))\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(256, 128), nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.gap(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize(tensor, mean, std):\n    mean = torch.tensor(mean).view(1, -1, 1, 1).to(tensor.device)\n    std  = torch.tensor(std).view(1, -1, 1, 1).to(tensor.device)\n    img  = tensor * std + mean\n    return torch.clamp(img, 0.0, 1.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]   # example: ImageNet \nstd  = [0.229, 0.224, 0.225] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_psnr_ssim(pred, target):\n   \n    # Denormalize to [0,1]\n    pred = denormalize(pred, mean, std)\n    target = denormalize(target, mean, std)\n    \n    ssim_val = ssim(pred, target, data_range=1.0)\n\n    psnr_val = psnr(pred, target, data_range=1.0)\n\n    return psnr_val.item(), ssim_val.item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_noisy_denoised_clean(loader, n=6):\n    denoised, labels, noisy_paths, clean_paths = next(iter(loader))\n\n    simple_transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor()\n    ])\n\n    fig, axes = plt.subplots(3, n, figsize=(3 * n, 9))\n\n    for i in range(n):\n        noisy_img = Image.open(noisy_paths[i]).convert(\"RGB\")\n        noisy_img = simple_transform(noisy_img).permute(1, 2, 0).numpy()\n        axes[0, i].imshow(noisy_img)\n        axes[0, i].set_title(\"Original Noisy\")\n        axes[0, i].axis(\"off\")\n\n        den_img = denormalize(denoised[i].unsqueeze(0), mean, std)\n        den_img = den_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n        axes[1, i].imshow(den_img)\n        axes[1, i].set_title(\"Denoised\")\n        axes[1, i].axis(\"off\")\n\n        clean_img = Image.open(clean_paths[i]).convert(\"RGB\")\n        clean_img = simple_transform(clean_img).permute(1, 2, 0).numpy()\n        axes[2, i].imshow(clean_img)\n        axes[2, i].set_title(\"Original Clean\")\n        axes[2, i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_denoiser(model, train_loader, val_loader,epochs, alpha=0.8, beta=0.2):\n\n    l1_loss = nn.L1Loss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5)\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    for epoch in range(epochs):\n        model.train()\n        total_train_loss, train_psnr, train_ssim = 0, [], []\n\n        for noisy, clean, *_ in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n            noisy, clean = noisy.to(device), clean.to(device)\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                denoised = model(noisy)\n\n                l1 = l1_loss(denoised, clean)\n\n                denoised_dn = denormalize(denoised, mean, std)\n                clean_dn    = denormalize(clean, mean, std)\n                ssim_val = ssim(denoised_dn, clean_dn, data_range=1.0)\n                ssim_l = 1 - ssim_val\n\n                loss = alpha * l1 + beta * ssim_l\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_train_loss += loss.item()\n\n            psnr_val, ssim_val_eval = find_psnr_ssim(clean, denoised)\n            train_psnr.append(psnr_val)\n            train_ssim.append(ssim_val_eval)\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        avg_train_psnr = sum(train_psnr) / len(train_psnr)\n        avg_train_ssim = sum(train_ssim) / len(train_ssim)\n\n        model.eval()\n        total_val_loss, val_psnr, val_ssim = 0, [], []\n        with torch.no_grad():\n            for noisy, clean, *_ in tqdm(val_loader, desc=\"Validation\"):\n                noisy, clean = noisy.to(device), clean.to(device)\n\n                with torch.cuda.amp.autocast():\n                    denoised = model(noisy)\n\n                    l1 = l1_loss(denoised, clean)\n                    denoised_dn = denormalize(denoised, mean, std)\n                    clean_dn    = denormalize(clean, mean, std)\n                    ssim_val = ssim(denoised_dn, clean_dn, data_range=1.0)\n                    ssim_l = 1 - ssim_val\n                    val_loss = alpha * l1 + beta * ssim_l\n\n                total_val_loss += val_loss.item()\n                \n                psnr_val, ssim_val_eval = find_psnr_ssim(clean, denoised)\n                val_psnr.append(psnr_val)\n                val_ssim.append(ssim_val_eval)\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        avg_val_psnr = sum(val_psnr) / len(val_psnr)\n        avg_val_ssim = sum(val_ssim) / len(val_ssim)\n\n        scheduler.step(avg_val_loss)\n\n        print(f\"[Epoch {epoch+1}/{epochs}] \"\n              f\"Train Loss: {avg_train_loss:.4f} | \"\n              f\"Train PSNR: {avg_train_psnr:.3f} | Train SSIM: {avg_train_ssim:.3f} \"\n              f\"|| Val Loss: {avg_val_loss:.4f} | \"\n              f\"Val PSNR: {avg_val_psnr:.3f} | Val SSIM: {avg_val_ssim:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_classifier(classifier, train_loader, val_loader, optimizer, epochs=200):\n    loss_fn = nn.CrossEntropyLoss()\n\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True, min_lr=1e-6\n    )\n\n    for epoch in range(epochs):\n        # Training\n        classifier.train()\n        total_loss, correct, total = 0, 0, 0\n        for imgs,labels,*_ in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n            imgs,labels=imgs.to(device),labels.to(device)\n            optimizer.zero_grad()\n            preds = classifier(imgs)\n            loss = loss_fn(preds, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            correct += (preds.argmax(1) == labels).sum().item()\n            total += labels.size(0)\n\n        train_loss = total_loss / len(train_loader)\n        train_acc = correct / total\n        train_losses.append(train_loss)\n        train_accuracies.append(train_acc)\n\n        # Validation\n        classifier.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for imgs,labels,*_ in val_loader:\n                imgs,labels  = imgs.to(device),labels.to(device)\n                preds = classifier(imgs)\n                loss = loss_fn(preds, labels)\n\n                val_loss += loss.item()\n                val_correct += (preds.argmax(1) == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n\n        scheduler.step(val_loss)\n\n        print(f\"[Epoch {epoch+1}/{epochs}] \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DenoisedDataset(Dataset):\n    def __init__(self, denoiser, base_loader):\n        self.samples = []\n        self.denoiser = denoiser.eval()  # eval mode\n\n        device = next(denoiser.parameters()).device\n\n        with torch.no_grad():\n            for noisy, clean, noisy_path, clean_path, labels in base_loader:\n                noisy = noisy.to(device)\n                denoised = denoiser(noisy).cpu()   # [B, C, H, W]\n\n                for d, l, np, cp in zip(denoised, labels, noisy_path, clean_path):\n                    self.samples.append((d, l, np, cp))  \n                    # (denoised tensor, label, noisy path, clean path)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]  # (denoised, label, noisy_path, clean_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((128, 128)),\n    #transforms.RandomHorizontalFlip(),\n    #transforms.RandomRotation(10),\n    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                         ]) #\n\nval_transforms = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = \"/kaggle/input/180-dc-ml-sig-recruitment/REC_DATASET/train\"\ntest_set  = \"/kaggle/input/180-dc-ml-sig-recruitment/REC_DATASET/test/noisy\"\n\ndataset = FlowerDataset(train_set, transform=train_transforms)\nval_split = 0.2\nval_size = int(len(dataset)*val_split)\ntrain_size = len(dataset)-val_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_psnr, baseline_ssim = 0, 0\nfor noisy, clean, *_ in train_loader:\n    b_psnr, b_ssim = find_psnr_ssim(noisy, clean)\n    baseline_psnr += b_psnr\n    baseline_ssim += b_ssim\n\nbaseline_psnr /= len(train_loader)\nbaseline_ssim /= len(train_loader)\n\nprint(f\"Baseline PSNR (noisy vs clean): {baseline_psnr:.2f}\")\nprint(f\"Baseline SSIM (noisy vs clean): {baseline_ssim:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for noisy, clean, noisy_paths, clean_paths,label in train_loader:\n    print(\"Noisy batch shape:\", noisy.shape)\n    print(\"Clean batch shape:\", clean.shape)\n    print(\"First noisy path:\", noisy_paths[0])\n    print(\"First clean path:\", clean_paths[0])\n    print(\"First label: \", label[0])\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"denoiser = UNetAutoencoder().to(device)\ntrain_denoiser(denoiser, train_loader, val_loader,epochs=75)\ntorch.save(denoiser.state_dict(), \"denoiser.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"denoiser.load_state_dict(torch.load(\"denoiser.pth\"))\ndenoised_train = DenoisedDataset(denoiser, train_loader)\ndenoised_val   = DenoisedDataset(denoiser, val_loader)\ntrain_loader_d = DataLoader(denoised_train, batch_size=32, shuffle=True)\nval_loader_d   = DataLoader(denoised_val, batch_size=32, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_noisy_denoised_clean(train_loader_d)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifier = StrongCNN().to(device)\noptimizer_c = optim.Adam(classifier.parameters(), lr=5e-5,weight_decay=1e-4)\ntrain_loss,val_loss,train_acc,val_acc = train_classifier(classifier, train_loader_d, val_loader_d, optimizer_c)\ntorch.save(classifier.state_dict(), \"classifier.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef generate_submission(denoiser, classifier, test_set, out_file=\"submission.csv\"):\n    class TestDataset(Dataset):\n        def __init__(self, root_dir, transform=None):\n            self.noisy_dir = root_dir\n            self.images = sorted(os.listdir(root_dir))\n            self.transform = transform\n\n        def __len__(self): return len(self.images)\n\n        def __getitem__(self, idx):\n            path = os.path.join(self.noisy_dir, self.images[idx])\n            img = Image.open(path).convert(\"RGB\")\n            if self.transform: img = self.transform(img)\n            return img, self.images[idx]\n\n    test_dataset = TestDataset(test_set, transform=val_transforms)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    denoiser.eval(); classifier.eval()\n    predictions, filenames = [], []\n\n    with torch.no_grad():\n        for imgs, names in test_loader:\n            imgs = imgs.to(device)\n            denoised = denoiser(imgs)\n            preds = classifier(denoised)\n            preds = preds.argmax(1).cpu().numpy()  # 0–4\n            preds = preds + 1  # shift to 1–5\n            predictions.extend(preds)\n            filenames.extend(names)\n\n    df = pd.DataFrame({\"Images\": filenames, \"Predicted_Classes\": predictions})\n    df.to_csv(out_file, index=False)\n    print(f\"Saved submission to {out_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.utils import save_image\n\ndef save_denoised_images(denoiser, test_set, output_dir=\"denoised_outputs\"):\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Test dataset \n    class TestDataset(Dataset):\n        def __init__(self, root_dir, transform=None):\n            self.noisy_dir = root_dir\n            self.images = sorted(os.listdir(root_dir))\n            self.transform = transform\n\n        def __len__(self): return len(self.images)\n\n        def __getitem__(self, idx):\n            path = os.path.join(self.noisy_dir, self.images[idx])\n            img = Image.open(path).convert(\"RGB\")\n            if val_transforms: img = val_transforms(img)\n            return img, self.images[idx]\n\n    test_dataset = TestDataset(test_set, transform=val_transforms)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    denoiser.eval()\n    with torch.no_grad():\n        for imgs, names in test_loader:\n            imgs = imgs.to(device)\n            denoised = denoiser(imgs).cpu()\n            for img, name in zip(denoised, names):\n                save_path = os.path.join(output_dir, os.path.splitext(name)[0] + \".png\")\n                save_image(img, save_path)\n\n    print(f\"Denoised images saved to: {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_denoised_images(denoiser, test_set, output_dir=\"denoised_test_images\")\n\ngenerate_submission(denoiser, classifier, test_set, out_file=\"submission.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}